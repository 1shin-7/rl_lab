深度强化学习实践

指导教师：钟梁

一，目标：

通过实习系统掌握深度强化学习原理，熟练运用深度强化学习开发工具完成算法设计；将理论转化为工程应用能力，明确实习核心方向，确保有序推进，夯实专业技能基础。

实习采取“问题+分组”的形式。首先由老师提供思考题，然后由学生分组讨论，最后分组汇报。老师感觉大家基本掌握之后，转入下一个环节的学习。

二，题目及要求：（**要求完成以下课题，分组汇报，独立撰写报告）**

**思考题1：深度强化学习基础知识**

了解深度强化学习的基本概念和深度强化学习的发展演进等基本知识。

**算法设计题：[使用Q-learning解决悬崖寻路问题](https://datawhalechina.github.io/easy-rl/%22%20%5Cl%20%22/chapter3/project1?id=%e4%bd%bf%e7%94%a8q-learning%e8%a7%a3%e5%86%b3%e6%82%ac%e5%b4%96%e5%af%bb%e8%b7%af%e9%97%ae%e9%a2%98)：**

**[CliffWalking-v0环境简介](https://datawhalechina.github.io/easy-rl/%22%20%5Cl%20%22/chapter3/project1?id=cliffwalking-v0%e7%8e%af%e5%a2%83%e7%ae%80%e4%bb%8b)**

环境中文名称叫悬崖寻路问题（CliffWalking），是指在一个4 x 12的网格中，智能体以网格的左下角位置为起点，以网格的右下角位置为终点，目标是移动智能体到达终点位置，智能体每次可以在上、下、左、右这4个方向中移动一步，每移动一步会得到-1单位的奖励。

![](data:image/png;base64...)

如图，红色部分表示悬崖，数字代表智能体能够观测到的位置信息，即observation，总共会有0-47等48个不同的值，智能体在移动中会有以下限制：

智能体不能移出网格，如果智能体想执行某个动作移出网格，那么这一步智能体不会移动，但是这个操作依然会得到-1单位的奖励

如果智能体“掉入悬崖” ，会立即回到起点位置，并得到-100单位的奖励

当智能体移动到终点时，该回合结束，该回合总奖励为各步奖励之和

任务要求：

训练并绘制reward以及滑动平均后的reward随episode的变化曲线图并记录超参数。

**算法设计题：[使用DQN实现CartPole-v0](https://datawhalechina.github.io/easy-rl/%22%20%5Cl%20%22/chapter7/project2?id=%e4%bd%bf%e7%94%a8dqn%e5%ae%9e%e7%8e%b0cartpole-v0)：**

CartPole-v0是OpenAI gym中的一个经典环境，通过向左或向右推车能够实现平衡，所以动作空间由两个动作组成。

![p1](data:image/png;base64...)

任务要求：

训练并绘制reward以及滑动平均后的reward随episode的变化曲线图并记录超参数。

**算法设计题：基于深度强化学习的机器人避障与路径规划：**

1. 项目背景

移动机器人的自主导航是机器人领域的核心技术之一。传统的路径规划算法在已知静态地图中表现良好，但在环境未知或存在动态障碍物的复杂场景下，往往缺乏适应性。深度强化学习通过让智能体在与环境的交互中试错学习，能够实现端到端的感知与决策。本项目旨在利用深度强化学习算法解决移动机器人的局部避障与导航问题，实现从传感器输入到动作输出的智能控制。

2. 项目目标

理解原理：掌握马尔可夫决策过程（MDP）的建模方法（状态空间、动作空间、奖励函数）。

掌握算法：熟练掌握至少一种主流DRL算法（如 DQN, DDPG, TD3, SAC 或 PPO）。

工程实践：使用仿真环境（如 OpenAI Gym, PyBullet, 或 ROS-Gazebo）进行模型训练与测试。

分析评估：能够分析训练曲线，解决稀疏奖励、收敛困难等常见问题。

3. 任务描述

在一个二维仿真环境中，控制一个移动机器人从起点移动到目标点，同时避开环境中的静态障碍物。

基础要求：使用 DQN (Deep Q-Network) 或其变体（Double DQN, Dueling DQN）处理离散动作空间。

进阶要求：使用 DDPG, TD3, SAC 或 PPO 处理连续动作空间，实现更平滑的运动控制。

4. 技术栈与工具推荐

方案一：轻量级仿真

语言：Python

框架：PyTorch 或 TensorFlow

环境：OpenAI Gym (现 Gymnasium)

推荐库：gym-minigrid (离散网格) 或 自行编写简单的基于 pygame/matplotlib 的连续2D环境。

方案二：物理引擎仿真

环境：PyBullet 或 ROS + Gazebo (TurtleBot3)

特点：更接近真实物理世界，具有惯性和摩擦力，难度较大但含金量高。