# 深度强化学习实习报告

## 一、 实习目标与技术选型

本次实习的主要目标是深入理解深度强化学习（DRL）的原理，并亲手实现一个能解决经典控制问题的算法模型。

在开始动手之前，我进行了一番技术选型。虽然市面上有像 Stable Baselines3 这样成熟的库，但我认为作为学习者，从零开始实现算法更有助于理解核心逻辑。因此，我选择了 **PyTorch** 作为深度学习框架，因为它调试起来非常直观，方便查看张量（Tensor）的形状和梯度。环境方面，自然是选择了标准的 **Gymnasium**。为了方便观察训练状态，我还尝试用 **Textual** 写了一个简单的终端界面（TUI），这样就能实时看到小车有没有倒，而不用一直盯着打印的 Log 看。

## 二、 实践过程：从碰壁到突破

### 1. 搭建基础版 DQN
一开始，我按照课本上的原理，搭建了一个最基础的 Deep Q-Network。简单来说，就是用一个神经网络来代替 Q-Table。我首先在 `CliffWalking`（悬崖寻路）和 `CartPole`（倒立摆）这两个环境上进行了测试。

起初，训练效果并不理想。在 CartPole 任务中，Loss 曲线震荡得非常厉害，Reward 也忽高忽低。我检查了很久代码，后来发现是目标网络（Target Network）的更新频率设置得太低了，导致训练目标一直在变，网络根本学不稳。把更新间隔调大之后，曲线终于开始收敛了。

### 2. 遇到的困难与“意外发现”
在处理 CliffWalking 任务时，我遇到了一个棘手的问题：模型虽然能学会避开悬崖，但收敛速度很慢，而且有时候会莫名其妙地在安全区域原地打转。

我当时非常困惑，就在 Google 上搜索“DQN 收敛慢”、“DQN oscillating”等关键词，查阅了很多技术博客和论坛。在阅读一篇关于 DQN 改进技巧的文章时，我意外发现了一个叫 **“过估计（Overestimation）”** 的概念。原来，普通的 DQN 因为总是取最大值（max Q），容易高估某些动作的价值，导致策略跑偏。

顺着这个线索，我找到了一篇经典的论文 *Deep Reinforcement Learning with Double Q-learning*。论文里提到的 **Double DQN (DDQN)** 刚好就是解决这个问题的。它的思路其实很简单：用当前网络选动作，用目标网络算分数，这样就避免了自卖自夸。

既然都要改了，我又顺藤摸瓜看了另一篇关于 **Dueling Network** 的论文。它把网络分成了两路，一路看“状态好不好”，一路看“动作对不对”。我觉得这个思路对 CartPole 这种状态比较连续的任务应该很有用。

于是，我参考着论文里的公式，修改了 `agent.py` 和 `models.py`：
*   把 `replay` 函数里的 Q 值计算逻辑改成了 DDQN 的形式。
*   把简单的全连接网络改成了 Dueling 结构。

改完之后重新训练，效果立竿见影！CliffWalking 的收敛速度明显变快了，Agent 的行为也果断了很多。

### 3. CartPole 的“偏科”问题
在 CartPole 训练后期，我又发现了一个奇怪的现象：杆子虽然不倒，但小车总是喜欢一直往右跑，直到撞到边界结束。虽然这样也能拿满分（因为 Gym 的奖励只看活多久），但这显然不是完美的平衡。

我想，这应该是奖励设置的问题。环境只给了“存活奖励”，没告诉小车“要待在中间”。于是我自己写了一个 `CenteredRewardWrapper`，给奖励函数加了一个惩罚项：离中心越远，扣分越多。加上这个约束后，小车终于学会了在屏幕中间稳稳地晃悠。

## 三、 收获与总结

这次实习经历让我明白，强化学习不仅仅是写好代码、跑通算法那么简单。

1.  **理论与实践的距离**：书上说“DQN 能收敛”，但实际操作中，Learning Rate、Batch Size 甚至网络结构的一点点微调，都会极大影响结果。
2.  **解决问题的能力**：这次最宝贵的经验就是通过搜索和查阅文献，解决了过估计的问题。这种“遇到问题 -> 查论文 -> 复现算法”的过程，让我对科研和工程实践有了更真实的体会。
3.  **工具的重要性**：自己写的那个可视化 TUI 帮了大忙，让我能直观地看到 Agent 到底是在学东西，还是在瞎蒙。

总的来说，从最开始看着代码报错头大，到最后看着 Agent 在屏幕里灵活地平衡杆子，这种成就感是非常真实的。

## 四、 参考论文

1.  Mnih, V., et al. (2015). **Human-level control through deep reinforcement learning**. *Nature*. (DQN 基础)
2.  Van Hasselt, H., Guez, A., & Silver, D. (2016). **Deep Reinforcement Learning with Double Q-learning**. *AAAI*. (DDQN)
3.  Wang, Z., et al. (2016). **Dueling Network Architectures for Deep Reinforcement Learning**. *ICML*. (Dueling DQN)
